{
  "model": "gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "Create an app that uses javascript ES6, import not require, fetch and node scripts in .mjs. Create client scripts in a client folder e.g. client/index.html and server scripts in a server folder e.g. server/server.mjs. For each script including launch.json provide the filename preceded by \"### File: \". On the next line show the code. On the next line show the explanation of what the code does. When importing javascript files use the filename and extension. Show the folder structure diagram including the filenames.Please provide instructions for installing node modules and for execution and debugging for client and server in VSCode.  For these instructions provide a heading of \"### Instructions: \". We are  using node version 20+. Use this to create an application that allows the upload of .ics file then use it as a prompt and let me query it using gbt 3.5-turbo using my OpenAI key. Use the API URL= https://api.openai.com/v1/chat/completions.\n"
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "Sure, I'\''ll create a simple application as requested. This app will allow the user to upload an `.ics` file and use it as a prompt to query the GPT-3.5-turbo model using the OpenAI API. Below is the complete instruction and necessary code to achieve this.\n\n### Folder Structure:\n```\nproject-folder/\n|-- client/\n|   |-- index.html\n|   |-- script.js\n|-- server/\n|   |-- server.mjs\n|-- package.json\n|-- launch.json\n```\n\n### Instructions:\n1. **Install Node Modules:**\n    - Open a terminal and navigate to `project-folder`.\n    - Run: `npm init -y` (to create `package.json` if not exists)\n    - Run: `npm install express multer node-fetch`\n\n2. **Execution and Debugging:**\n    - Open `project-folder` in VSCode.\n    - Make sure your `launch.json` is correctly configured for debugging.\n    - Use the provided `launch.json` to start the server.\n    - Open `client/index.html` using Live Server extension in VSCode or any other way to serve static files.\n\n### File: launch.json\n```json\n{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"type\": \"node\",\n      \"request\": \"launch\",\n      \"name\": \"Launch Server\",\n      \"program\": \"${workspaceFolder}/server/server.mjs\"\n    }\n  ]\n}\n```\nThis file configures VSCode to debug the Node.js server file `server/server.mjs`.\n\n### File: package.json\n```json\n{\n  \"name\": \"ics-upload-gpt3\",\n  \"version\": \"1.0.0\",\n  \"main\": \"server/server.mjs\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"start\": \"node server/server.mjs\"\n  },\n  \"dependencies\": {\n    \"express\": \"^4.18.2\",\n    \"multer\": \"^1.4.4\",\n    \"node-fetch\": \"^3.3.0\"\n  }\n}\n```\nThis file defines the metadata for the project and its dependencies.\n\n### File: client/index.html\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>ICS Upload</title>\n</head>\n<body>\n    <h1>Upload ICS File</h1>\n    <form id=\"uploadForm\" enctype=\"multipart/form-data\">\n        <input type=\"file\" id=\"fileInput\" name=\"icsFile\" accept=\".ics\" required>\n        <button type=\"submit\">Upload</button>\n    </form>\n    <pre id=\"response\"></pre>\n\n    <script src=\"script.js\"></script>\n</body>\n</html>\n```\nThis is the HTML file for the client side. It includes a form to allow users to upload an `.ics` file and a script to handle the form submission.\n\n### File: client/script.js\n```javascript\ndocument.getElementById('\''uploadForm'\'').addEventListener('\''submit'\'', async (event) => {\n    event.preventDefault();\n\n    const fileInput = document.getElementById('\''fileInput'\'');\n    const file = fileInput.files[0];\n\n    const formData = new FormData();\n    formData.append('\''icsFile'\'', file);\n\n    const response = await fetch('\''/upload'\'', {\n        method: '\''POST'\'',\n        body: formData\n    });\n\n    const data = await response.json();\n    document.getElementById('\''response'\'').textContent = JSON.stringify(data, null, 2);\n});\n```\nThis JavaScript code handles the form submission, sends the `.ics` file to the server, and displays the response from the server.\n\n### File: server/server.mjs\n```javascript\nimport express from '\''express'\'';\nimport multer from '\''multer'\'';\nimport fetch from '\''node-fetch'\'';\nimport fs from '\''fs'\'';\nimport path from '\''path'\'';\n\nconst app = express();\nconst upload = multer({ dest: '\''uploads/'\'' });\n\napp.use(express.static('\''client'\''));\n\napp.post('\''/upload'\'', upload.single('\''icsFile'\''), async (req, res) => {\n    const icsFilePath = path.join('\''uploads'\'', req.file.filename);\n    const icsFileContent = fs.readFileSync(icsFilePath, '\''utf-8'\'');\n\n    const openaiResponse = await fetch('\''https://api.openai.com/v1/chat/completions'\'', {\n        method: '\''POST'\'',\n        headers: {\n            '\''Authorization'\'': `Bearer YOUR_OPENAI_API_KEY`,\n            '\''Content-Type'\'': '\''application/json'\''\n        },\n        body: JSON.stringify({\n            model: '\''gpt-3.5-turbo'\'',\n            messages: [{ role: '\''user'\'', content: icsFileContent }]\n        })\n    });\n\n    const openaiData = await openaiResponse.json();\n    res.json(openaiData);\n});\n\nconst PORT = process.env.PORT || 3000;\napp.listen(PORT, () => {\n    console.log(`Server is running on http://localhost:${PORT}`);\n});\n```\nThis is the server code using Express.js to handle file uploads and act as a proxy to send the `.ics` file content as a prompt to the OpenAI API. Replace `YOUR_OPENAI_API_KEY` with your actual OpenAI API key.\n\nNow, with everything set up, you should be able to start the server and upload `.ics` files via the client interface, which will prompt the OpenAI API using the content of the uploaded file and display the response."
        }
      ]
    }
  ],
  "temperature": 1,
  "max_tokens": 4070,
  "top_p": 1,
  "frequency_penalty": 0,
  "presence_penalty": 0
}'